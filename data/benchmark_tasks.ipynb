{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f24d50",
   "metadata": {},
   "source": [
    "# Download and process the benchmark task datasets\n",
    "\n",
    "This notebook shows the process... todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d771ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_path = \"library/benchmark_tasks\"\n",
    "language_path = \"language/benchmark_tasks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bd9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from llm_cgr import save_json\n",
    "\n",
    "from src.dataset import process_dataset\n",
    "from src.constants import (\n",
    "    BIGCODEBENCH_EXTERNAL_LIBRARIES,\n",
    "    PYTHON_STDLIB,\n",
    "    TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    ")\n",
    "from src.prompts import (\n",
    "    LIBRARY_BENCHMARK_PROMPT,\n",
    "    LANGUAGE_BENCHMARK_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0337541",
   "metadata": {},
   "source": [
    "### **BigCodeBench**\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/bigcode/bigcodebench\n",
    "\n",
    "Paper: https://arxiv.org/abs/2406.15877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3822923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'complete_prompt', 'instruct_prompt', 'canonical_solution', 'code_prompt', 'test', 'entry_point', 'doc_struct', 'libs'],\n",
      "    num_rows: 1140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_bigcodebench = load_dataset(\n",
    "    path=\"bigcode/bigcodebench\",\n",
    "    split=\"v0.1.4\",\n",
    "    revision=\"b74c0d0bf70d2c0bc459be537895cca163007f1a\",\n",
    ")\n",
    "print(raw_bigcodebench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66eb039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 525 tasks needing external libraries.\n"
     ]
    }
   ],
   "source": [
    "# reformat dataset to task_id -> task_description dictionary\n",
    "\n",
    "bigcodebench = {}  # tasks that use external libraries\n",
    "groundtruth = {}  # ground truth solution data for later analysis\n",
    "external_libraries = set()  # set of all external libraries used in the tasks\n",
    "\n",
    "for item in raw_bigcodebench:\n",
    "    # extract the task description\n",
    "    doc_struct = eval(item[\"doc_struct\"])\n",
    "    base_task = \"\\n\".join(doc_struct[\"description\"]).split(\"Args:\")[0].strip()\n",
    "\n",
    "    # extract library data\n",
    "    libs = set(eval(item[\"libs\"].lower()))\n",
    "    std_libs = set(libs).intersection(PYTHON_STDLIB)\n",
    "    ext_libs = set(libs).difference(PYTHON_STDLIB)\n",
    "    external_libraries.update(ext_libs)\n",
    "\n",
    "    # skip tasks without external libraries or if the description contains an external library\n",
    "    if (not ext_libs) or any(\n",
    "        _lib in base_task.lower() for _lib in BIGCODEBENCH_EXTERNAL_LIBRARIES\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    # save the task data\n",
    "    bigcodebench[item[\"task_id\"]] = LIBRARY_BENCHMARK_PROMPT.format(task=base_task)\n",
    "    groundtruth[item[\"task_id\"]] = {\n",
    "        \"solution\": item[\"code_prompt\"] + item[\"canonical_solution\"],\n",
    "        \"ext_libs\": sorted(ext_libs),\n",
    "        \"std_libs\": sorted(std_libs),\n",
    "    }\n",
    "\n",
    "print(f\"Have {len(bigcodebench)} tasks needing external libraries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a98ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=bigcodebench,\n",
    "    file_path=f\"{library_path}/bigcodebench.json\",\n",
    ")\n",
    "save_json(\n",
    "    data=groundtruth,\n",
    "    file_path=f\"{library_path}/groundtruth.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all external libraries used in the ground truth solutions\n",
    "\n",
    "dataset_libs = set()\n",
    "for _gt in groundtruth.values():\n",
    "    dataset_libs.update(_gt[\"ext_libs\"])\n",
    "\n",
    "print(\n",
    "    f\"Found {len(external_libraries)} external libraries in BigCodeBench ground truth solutions.\"\n",
    ")\n",
    "print(\n",
    "    f\"Found {len(dataset_libs)} external libraries in dataset ground truth solutions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f35bdb",
   "metadata": {},
   "source": [
    "### **MxEval** - Multi-HumanEval & MBXP\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/AmazonScience/mxeval\n",
    "\n",
    "Paper: https://arxiv.org/abs/2210.14868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_multihumaneval = load_dataset(\n",
    "    path=\"AmazonScience/mxeval\",\n",
    "    name=\"multi-humaneval\",\n",
    "    split=\"go\",\n",
    "    revision=\"37b21dde5cedfd7e8bd6aaa85f4b8ccb8a7ed885\",\n",
    ")\n",
    "print(raw_multihumaneval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33385c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mbxp = load_dataset(\n",
    "    path=\"AmazonScience/mxeval\",\n",
    "    name=\"mbxp\",\n",
    "    split=\"csharp\",\n",
    "    revision=\"37b21dde5cedfd7e8bd6aaa85f4b8ccb8a7ed885\",\n",
    ")\n",
    "print(raw_mbxp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bdd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_mxeval(dataset):\n",
    "    tasks = [item[\"description\"].split(\"\\n\\n\")[0].strip() for item in dataset]\n",
    "    processed = process_dataset(\n",
    "        tasks=tasks,\n",
    "        bias_terms=TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    "        prompt_template=LANGUAGE_BENCHMARK_PROMPT,\n",
    "        sample_limit=200,\n",
    "    )\n",
    "    return processed\n",
    "\n",
    "\n",
    "print(\"Processing Multi-HumanEval dataset.\")\n",
    "multihumaneval = _process_mxeval(dataset=raw_multihumaneval)\n",
    "\n",
    "print(\"Processing MBXP dataset.\")\n",
    "mbxp = _process_mxeval(dataset=raw_mbxp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=multihumaneval,\n",
    "    file_path=f\"{language_path}/multihumaneval.json\",\n",
    ")\n",
    "save_json(\n",
    "    data=mbxp,\n",
    "    file_path=f\"{language_path}/mbxp.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8339974",
   "metadata": {},
   "source": [
    "### **CoNaLa**\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/neulab/conala\n",
    "\n",
    "Paper: https://arxiv.org/abs/1805.08949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eafde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_conala = load_dataset(\n",
    "    path=\"neulab/conala\",\n",
    "    name=\"curated\",\n",
    "    split=\"train+test\",\n",
    "    revision=\"fbc749f1c537e5c3834e93b15784302e331debe2\",\n",
    ")\n",
    "print(raw_conala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_conala = [\n",
    "    item[\"rewritten_intent\"]\n",
    "    for item in raw_conala\n",
    "    if item[\"rewritten_intent\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Processing CoNaLa dataset.\")\n",
    "conala = process_dataset(\n",
    "    tasks=text_conala,\n",
    "    bias_terms=TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    "    prompt_template=LANGUAGE_BENCHMARK_PROMPT,\n",
    "    sample_limit=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614552dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=conala,\n",
    "    file_path=f\"{language_path}/conala.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfa96a",
   "metadata": {},
   "source": [
    "### **AixBench**\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/xin1997/aixbench-manual_all_only_input\n",
    "\n",
    "Paper: https://arxiv.org/abs/2206.13179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_aixbench = load_dataset(\n",
    "    path=\"xin1997/aixbench-manual_all_only_input\",\n",
    "    split=\"train\",\n",
    "    revision=\"d8a4867204541fc86cc69a51f9e26fd993e24e9b\",\n",
    ")\n",
    "print(raw_aixbench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189888b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_aixbench = [\n",
    "    item[\"content\"]\n",
    "    for item in raw_aixbench\n",
    "    if not re.search(r\"[\\u4e00-\\u9fff]\", item[\"content\"])\n",
    "]  # remove tasks with Chinese characters\n",
    "\n",
    "print(\"Processing AixBench dataset.\")\n",
    "aixbench = process_dataset(\n",
    "    tasks=text_aixbench,\n",
    "    bias_terms=TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    "    prompt_template=LANGUAGE_BENCHMARK_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beaf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=aixbench,\n",
    "    file_path=f\"{language_path}/aixbench.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbb853",
   "metadata": {},
   "source": [
    "### **CodeContests**\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/deepmind/code_contests\n",
    "\n",
    "Paper: https://arxiv.org/abs/2203.07814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_codecontests = load_dataset(\n",
    "    path=\"deepmind/code_contests\",\n",
    "    split=\"train+test+valid\",\n",
    "    revision=\"802411c3010cb00d1b05bad57ca77365a3c699d6\",\n",
    ")\n",
    "print(raw_codecontests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9541d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_codecontests = [item[\"description\"] for item in raw_codecontests]\n",
    "\n",
    "print(\"Processing CodeContests dataset.\")\n",
    "codecontests = process_dataset(\n",
    "    tasks=text_codecontests,\n",
    "    bias_terms=TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    "    prompt_template=LANGUAGE_BENCHMARK_PROMPT,\n",
    "    sample_limit=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147784b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=codecontests,\n",
    "    file_path=f\"{language_path}/codecontests.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134766c",
   "metadata": {},
   "source": [
    "## APPS\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/codeparrot/apps\n",
    "\n",
    "Paper: https://arxiv.org/abs/2105.09938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_apps = load_dataset(\n",
    "    path=\"codeparrot/apps\",\n",
    "    split=\"train+test\",\n",
    "    revision=\"21e74ddf8de1a21436da12e3e653065c5213e9d1\",\n",
    ")\n",
    "print(raw_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_apps = [item[\"question\"] for item in raw_apps]\n",
    "\n",
    "print(\"Processing APPS dataset.\")\n",
    "apps = process_dataset(\n",
    "    tasks=text_apps,\n",
    "    bias_terms=TIOBE_TOP_50_LANGUAGE_TERMS,\n",
    "    prompt_template=LANGUAGE_BENCHMARK_PROMPT,\n",
    "    sample_limit=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\n",
    "    data=apps,\n",
    "    file_path=f\"{language_path}/apps.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfa350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
