{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run the experiments!** ðŸ¤–\n",
    "\n",
    "Call the methods to generate and analyse LLM responses for all experiments.\n",
    "\n",
    "Experiments described in **Section 3** of the paper, results given in **Section 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to configure models\n",
    "models = [\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"claude-3-5-sonnet-20241022\",\n",
    "    \"claude-3-5-haiku-20241022\",\n",
    "    \"meta-llama/llama-3.2-3b-instruct-turbo\",\n",
    "    \"qwen/qwen2.5-coder-32b-instruct\",\n",
    "    \"deepseek-ai/deepseek-llm-67b-chat\",\n",
    "    \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Preferences, Benchmark Tasks\n",
    "\n",
    "Analyse the libraries used by LLMs when solving library-agnostic python problems from BigCodeBench that require external libraries.\n",
    "\n",
    "Experiment described in **Section 3.3.1**, results given in **Section 4.1.1** and **Figure 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "\n",
    "run_llm_code_bias_experiment(\n",
    "    bias_type=\"library\",\n",
    "    dataset_file=\"data/library/benchmark_tasks/bigcodebench_ext.json\",\n",
    "    models=models,\n",
    "    samples=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Preferences, Project Initialisation Tasks\n",
    "\n",
    "Analyse the libraries used by LLMs when writing the initial structural code for new python projects that require external libraries.\n",
    "\n",
    "Experiment described in **Section 3.3.2**, results given in **Section 4.1.2** and **Table 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "\n",
    "for dataset in [\n",
    "    \"database\",\n",
    "    \"deeplearning\",\n",
    "    \"distributed\",\n",
    "    \"webscraper\",\n",
    "    \"webserver\",\n",
    "]:\n",
    "    run_llm_code_bias_experiment(\n",
    "        bias_type=\"language\",\n",
    "        dataset_file=f\"data/language/project_tasks/{dataset}.json\",\n",
    "        models=models,\n",
    "        samples=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Preferences, Benchmark Tasks\n",
    "\n",
    "Analyse the languages used by LLMs when solving language-agnostic coding problems from widely-used benchmark datasets.\n",
    "\n",
    "Experiment described in **Section 3.4.1**, results given in **Section 4.2.1** and **Table 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "\n",
    "for dataset in [\n",
    "    \"aixbench\",\n",
    "    \"codecontests\",\n",
    "    \"conala\",\n",
    "    \"leetcode\",\n",
    "    \"mbxp\",\n",
    "    \"multihumaneval\",\n",
    "]:\n",
    "    run_llm_code_bias_experiment(\n",
    "        bias_type=\"language\",\n",
    "        dataset_file=f\"data/language/benchmark_tasks/{dataset}.json\",\n",
    "        models=models,\n",
    "        samples=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Preferences, Project Initialisation Tasks\n",
    "\n",
    "Analyse the languages used by LLMs when writing the initial structural code for new projects.\n",
    "\n",
    "Experiment described in **Section 3.4.2**, results given in **Section 4.2.2** and **Table 5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "\n",
    "for dataset in [\n",
    "    \"concurrency\",\n",
    "    \"graphical\",\n",
    "    \"lowlatency\",\n",
    "    \"parallel\",\n",
    "    \"systemlevel\",\n",
    "]:\n",
    "    run_llm_code_bias_experiment(\n",
    "        bias_type=\"language\",\n",
    "        dataset_file=f\"data/language/project_tasks/{dataset}.json\",\n",
    "        models=models,\n",
    "        samples=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Temperature\n",
    "\n",
    "Analyse the languages and libraries used for writing initial project code when the temperature parameter is varied.\n",
    "\n",
    "Investigation done as part of the extended analysis in **Section 5.2.1**, results given in **Table 8**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "\n",
    "for dataset in [\n",
    "    \"concurrency\",\n",
    "    \"graphical\",\n",
    "    \"lowlatency\",\n",
    "    \"parallel\",\n",
    "    \"systemlevel\",\n",
    "]:\n",
    "    for temperature in [0.0, 0.5, 1.0, 1.5]:\n",
    "        run_llm_code_bias_experiment(\n",
    "            bias_type=\"language\",\n",
    "            dataset_file=f\"data/language/project_tasks/{dataset}.json\",\n",
    "            models=[\"gpt-4o-mini-2024-07-18\"],\n",
    "            samples=100,\n",
    "            temperature=temperature,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning via Prompt Engineering\n",
    "\n",
    "Analyse the languages used by LLMs when writing the initial structural code for new projects, when using a prompt designed to induce reasoning. Does it help to mitigate the internal inconsistencies?\n",
    "\n",
    "Investigation done as part of the extended analysis in **Section 5.2.2**, results given in **Table 9**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import run_llm_code_bias_experiment\n",
    "from src.prompts import (\n",
    "    LANGUAGE_POST_PROMPT_STEP,\n",
    "    LANGUAGE_POST_PROMPT_CHECK,\n",
    "    LANGUAGE_POST_PROMPT_LIST,\n",
    ")\n",
    "\n",
    "for dataset in [\n",
    "    \"concurrency\",\n",
    "    \"graphical\",\n",
    "    \"lowlatency\",\n",
    "    \"parallel\",\n",
    "    \"systemlevel\",\n",
    "]:\n",
    "    for reasoning_prompt in [\n",
    "        LANGUAGE_POST_PROMPT_CHECK,\n",
    "        LANGUAGE_POST_PROMPT_LIST,\n",
    "        LANGUAGE_POST_PROMPT_STEP,\n",
    "    ]:\n",
    "        run_llm_code_bias_experiment(\n",
    "            bias_type=\"language\",\n",
    "            dataset_file=f\"data/language/project_tasks/{dataset}.json\",\n",
    "            models=[\"gpt-4o-mini-2024-07-18\"],\n",
    "            samples=100,\n",
    "            post_prompt=reasoning_prompt,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
