{
    "metadata": {
        "run_id": "multihumaneval",
        "dataset_file": "data/language/benchmark_tasks/multihumaneval.json",
        "dataset_size": 160,
        "samples": 3,
        "total_tasks": 480,
        "configured_temperature": "None - used default",
        "configured_top_p": "None - used default",
        "configured_max_tokens": "None - used default",
        "start_datetime": "2025-02-06T15:15:45.886120",
        "end_datetime": "2025-02-06T23:24:57.540206"
    },
    "prompts": {
        "pre_prompt": null,
        "post_prompt": null
    },
    "evaluations": {
        "gpt-4o-mini-2024-07-18": {
            "task_counts": {
                "python": 160
            }
        },
        "gpt-3.5-turbo-0125": {
            "task_counts": {
                "python": 160
            }
        },
        "meta-llama/llama-3.2-3b-instruct-turbo": {
            "task_counts": {
                "python": 160
            }
        },
        "qwen/qwen2.5-coder-32b-instruct": {
            "task_counts": {
                "python": 160
            }
        },
        "deepseek-ai/deepseek-llm-67b-chat": {
            "task_counts": {
                "python": 160
            }
        },
        "mistralai/mistral-7b-instruct-v0.3": {
            "task_counts": {
                "python": 160,
                "php": 1,
                "c": 1
            }
        },
        "claude-3-5-sonnet-20241022": {
            "task_counts": {
                "python": 158,
                "javascript": 2
            }
        },
        "claude-3-5-haiku-20241022": {
            "task_counts": {
                "python": 159,
                "java": 1,
                "javascript": 2,
                "c": 1
            }
        }
    },
    "errors": {},
    "errors_fixed": true,
    "no_code_responses": {},
    "no_code_fixed": true
}
